# -*- coding: utf-8 -*-
"""NEWS_SUMMARY.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SdzKO858acKJVlPbMuyJVduGJYfvc70t
"""

import gdown

# Train file
gdown.download("https://drive.google.com/uc?id=1N083B7uOSGpYsH7ndBPNHmF-Z00vDexY", "train.csv", quiet=False)

# Validation file
gdown.download("https://drive.google.com/uc?id=16tKm4ZTbwEXVSfk0DB1p06OPUVD8EbJt", "validation.csv", quiet=False)

import pandas as pd

train_df = pd.read_csv("train.csv")
val_df = pd.read_csv("validation.csv")

print("Train shape:", train_df.shape)
print("Validation shape:", val_df.shape)
train_df.head()

train_df.dropna(subset=["article", "highlights"], inplace=True)
val_df.dropna(subset=["article", "highlights"], inplace=True)
# Take a small sample (e.g., 2000 for training, 500 for validation)
train_df = train_df.sample(n=2000, random_state=42).reset_index(drop=True)
val_df = val_df.sample(n=500, random_state=42).reset_index(drop=True)

import re

def clean_text(text):
    text = re.sub(r'\s+', ' ', text)             # multiple spaces
    text = re.sub(r'\[[^\]]*\]', '', text)       # remove [references]
    text = re.sub(r'https?://\S+', '', text)     # remove URLs
    text = re.sub(r'\s([?.!",](?:\s|$))', r'\1', text)  # space before punctuation
    return text.strip()

train_df["article"] = train_df["article"].astype(str).apply(clean_text)
train_df["highlights"] = train_df["highlights"].astype(str).apply(clean_text)

val_df["article"] = val_df["article"].astype(str).apply(clean_text)
val_df["highlights"] = val_df["highlights"].astype(str).apply(clean_text)

import seaborn as sns
import matplotlib.pyplot as plt

train_df['article_len'] = train_df['article'].apply(lambda x: len(x.split()))
train_df['summary_len'] = train_df['highlights'].apply(lambda x: len(x.split()))

sns.histplot(train_df['article_len'], bins=50, kde=True)
plt.title("Article Length Distribution")
plt.show()

sns.histplot(train_df['summary_len'], bins=50, kde=True)
plt.title("Summary Length Distribution")
plt.show()

train_df[['article', 'highlights']]
val_df[['article', 'highlights']]
train_df = train_df.drop_duplicates()
val_df = val_df.drop_duplicates()
train_df['article'] = train_df['article'].str.strip()
train_df['highlights'] = train_df['highlights'].str.strip()

val_df['article'] = val_df['article'].str.strip()
val_df['highlights'] = val_df['highlights'].str.strip()

from transformers import BartTokenizer

# Load the tokenizer
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')

# Sample encoding
sample = train_df.iloc[0]
input_text = sample['article']
summary_text = sample['highlights']

# Tokenize example
input_enc = tokenizer(
    input_text,
    max_length=1024,
    padding='max_length',
    truncation=True,
    return_tensors='pt'
)

summary_enc = tokenizer(
    summary_text,
    max_length=128,
    padding='max_length',
    truncation=True,
    return_tensors='pt'
)

from torch.utils.data import Dataset

class CNNDailyMailDataset(Dataset):
    def __init__(self, dataframe, tokenizer, input_max_len=1024, summary_max_len=128):
        self.tokenizer = tokenizer
        self.data = dataframe
        self.input_max_len = input_max_len
        self.summary_max_len = summary_max_len

    def __len__(self):
        return len(self.data)

    def __getitem__(self, index):
        article = self.data.iloc[index]['article']
        summary = self.data.iloc[index]['highlights']

        input_encodings = self.tokenizer(
            article,
            max_length=self.input_max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        summary_encodings = self.tokenizer(
            summary,
            max_length=self.summary_max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'input_ids': input_encodings.input_ids.squeeze(),  # shape: (1024,)
            'attention_mask': input_encodings.attention_mask.squeeze(),
            'labels': summary_encodings.input_ids.squeeze()
             }     # shape: (128,)

train_dataset = CNNDailyMailDataset(train_df, tokenizer)
val_dataset = CNNDailyMailDataset(val_df, tokenizer)

from torch.utils.data import DataLoader

# Set batch size (adjust based on your GPU capacity)
BATCH_SIZE = 4

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE)

batch = next(iter(train_loader))
print("Input IDs shape:", batch['input_ids'].shape)   # (BATCH_SIZE, 1024)
print("Labels shape:", batch['labels'].shape)         # (BATCH_SIZE, 128)

from transformers import BartForConditionalGeneration

# Load distilled BART model
model = BartForConditionalGeneration.from_pretrained('sshleifer/distilbart-cnn-12-6')

def tokenize_function(example):
    # Tokenize the article (input)
    model_inputs = tokenizer(
        example["article"],
        max_length=1024,
        padding="max_length",
        truncation=True
    )

    # Tokenize the highlights (target)
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            example["highlights"],
            max_length=128,
            padding="max_length",
            truncation=True
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs



from datasets import Dataset

train_dataset_hf = Dataset.from_pandas(train_df[['article', 'highlights']])
val_dataset_hf = Dataset.from_pandas(val_df[['article', 'highlights']])

train_tokenized = train_dataset_hf.map(tokenize_function, batched=True)
val_tokenized = val_dataset_hf.map(tokenize_function, batched=True)

from transformers import Seq2SeqTrainingArguments

training_args = Seq2SeqTrainingArguments(
    output_dir="./best_epoch1_model",         # Save folder
    num_train_epochs=1,                       # âœ… Only 1 epoch
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    eval_strategy="epoch",              # Evaluate after each epoch
    save_strategy="epoch",                    # âœ… Save after each epoch
    logging_strategy="steps",
    logging_steps=100,
    save_total_limit=1,                       # Keep only last checkpoint (epoch 1)
    predict_with_generate=True,
    fp16=True,                                # Use if you're on GPU
    report_to="none"                          # Disable WandB
)

from transformers import Seq2SeqTrainer

trainer = Seq2SeqTrainer(
    model=model,  # a fresh DistilBART/BART model
    args=training_args,
    train_dataset=train_tokenized,
    eval_dataset=val_tokenized,
    tokenizer=tokenizer
)

trainer.train()

import os

os.listdir("./results")

from transformers import BartForConditionalGeneration, BartTokenizer

model = BartForConditionalGeneration.from_pretrained("best_epoch1_model_manual")
tokenizer = BartTokenizer.from_pretrained("best_epoch1_model_manual")

model.to("cuda" if torch.cuda.is_available() else "cpu")

trainer.save_model("best_epoch1_model_manual")
tokenizer.save_pretrained("best_epoch1_model_manual")

import os
os.listdir("best_epoch1_model_manual")

article = val_df["article"].iloc[0]

inputs = tokenizer(article, return_tensors="pt", max_length=512, truncation=True).to(model.device)
summary_ids = model.generate(inputs["input_ids"], max_length=128, num_beams=4, early_stopping=True)
summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

print("ðŸ“° Original Article:\n", article[:500])
print("\nðŸ“¢ Generated Summary:\n", summary)

for i in range(5):
    art = val_df["article"].iloc[i]
    inputs = tokenizer(art, return_tensors="pt", max_length=512, truncation=True).to(model.device)
    summary_ids = model.generate(inputs["input_ids"], max_length=128, num_beams=4)
    summ = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
    print(f"\nðŸ“° Article #{i+1} Summary:\n{summ}")

!pip install rouge_score -q

from evaluate import load
rouge = load("rouge")

predictions = []
references = []

for i in range(50):  # use more if your GPU is fast enough
    article = val_df["article"].iloc[i]
    reference = val_df["highlights"].iloc[i]  # actual summary

    # Tokenize and generate
    inputs = tokenizer(article, return_tensors="pt", max_length=512, truncation=True).to(model.device)
    summary_ids = model.generate(inputs["input_ids"], max_length=128, num_beams=4, early_stopping=True)
    generated_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)

    predictions.append(generated_summary)
    references.append(reference)

results = rouge.compute(predictions=predictions, references=references, use_stemmer=True)

# Clean summary of main scores
for key in results:
    print(f"{key}: {results[key]:.4f}")

