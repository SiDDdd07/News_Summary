This project fine-tunes the lightweight transformer model DistilBART on the CNN/DailyMail news summarization dataset. The goal is to train a summarizer that can generate concise and meaningful summaries from lengthy news articles. The model was trained using the Hugging Face Seq2SeqTrainer framework, and achieved strong results after just 1 epoch of fine-tuning. ROUGE evaluation on the validation set shows that the model captures key information effectively, making it suitable for real-world summarization tasks. The project includes data cleaning, custom PyTorch dataset creation, tokenization, model training, and evaluation.
